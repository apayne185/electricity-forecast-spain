{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to use aprils data too to try to predict junes, however it considerably lowered the quality of the model: \n",
    "\n",
    "Loss: 0.0002993\n",
    "\n",
    "MAE: 0.005401\n",
    "\n",
    "RMSE: 0.01730\n",
    "\n",
    "R²: 0.6074\n",
    "\n",
    "Due to this, I am no longer using the models for it. However, may is an uncertain choice for predicting july's data. \n",
    "\n",
    "I also ruled out using relu, as it did not yield the results of tanh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import Sequence \n",
    "from tensorflow.keras.optimizers import Adam     \n",
    " \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,  mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omie_labelled = pd.read_csv('data\\\\df_omie_labelled.csv')\n",
    "filtered_categories = pd.read_csv('data\\\\filtered_categories.csv')\n",
    "df_omie_blind = pd.read_csv('data\\\\df_omie_blind.csv')\n",
    "unit_list = pd.read_csv('data\\\\unit_list.csv')\n",
    "\n",
    "\n",
    "unit_list[\"Tecnología\"] = unit_list[\"Tecnología\"].fillna(\"Unknown\")\n",
    "\n",
    "df_omie_labelled['fechaHora'] = pd.to_datetime(df_omie_labelled['fechaHora'])\n",
    "df_omie_blind['fechaHora'] = pd.to_datetime(df_omie_blind['fechaHora'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, train_months, test_month, columns_norm=[\"PrecEuro\", \"Energia\"]):\n",
    "    train_data = df[df['fechaHora'].dt.month.isin(train_months)]\n",
    "    test_data = df[df['fechaHora'].dt.month == test_month]\n",
    "     \n",
    "     \n",
    "    #fit scaler only on training data    \n",
    "    scaler = MinMaxScaler()  \n",
    "    train_data[columns_norm] = scaler.fit_transform(train_data[columns_norm])\n",
    "    test_data[columns_norm] = scaler.transform(test_data[columns_norm])\n",
    "    \n",
    "\n",
    "    return train_data, test_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergySequence(Sequence): \n",
    "    def __init__(self, data, target_col, sequence_length=24, batch_size=32):\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "         \n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.sequence_length) // self.batch_size \n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        X, y = [], []\n",
    "   \n",
    "\n",
    "        for i in range(start_idx, end_idx):\n",
    "            if i + self.sequence_length < len(self.data):\n",
    "                X.append(self.data.iloc[i : i + self.sequence_length].values)\n",
    "                y.append(self.data.iloc[i + self.sequence_length, self.target_col])\n",
    "\n",
    "        return np.array(X), np.array(y)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build Models \n",
    "(relu, tanh, combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(units, activations, input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(units[0], activation=activations[0], return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units[1], activation=activations[1]),\n",
    "        Dropout(0.2),\n",
    "        Dense(units[2], activation=activations[2]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)  \n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate Models\n",
    "\n",
    "trains on april-may, may data to predict junes data. From here I will determine which model (am/m using relu/tanh/combination) is best to predict july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(actual, predicted):\n",
    "    return 100 * np.mean(2 * np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted) + 1e-8))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, train_data, test_data, sequence_length, batch_size, epochs):\n",
    "    \n",
    "    train_generator = EnergySequence(train_data[['Energia', 'PrecEuro']], target_col=0, sequence_length=sequence_length, batch_size=batch_size)\n",
    "    test_generator = EnergySequence(test_data[['Energia', 'PrecEuro']], target_col=0, sequence_length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "    history = model.fit(train_generator, validation_data=test_generator, epochs=epochs, verbose=1)\n",
    "\n",
    "    loss = model.evaluate(test_generator)\n",
    "    \n",
    "    predictions = model.predict(test_generator)\n",
    "\n",
    "\n",
    "    test_targets = test_data['Energia'].iloc[sequence_length:sequence_length+len(predictions)].values\n",
    "    test_dates = test_data['fechaHora'].iloc[sequence_length:sequence_length+len(predictions)].values\n",
    "    \n",
    "    # error metrics\n",
    "    mae = mean_absolute_error(test_targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(test_targets, predictions))\n",
    "    r2 = r2_score(test_targets, predictions)\n",
    "    # mape = np.mean(np.abs((test_targets - predictions.flatten()) / test_targets)) * 100  \n",
    "    mape = smape(test_targets, predictions.flatten())\n",
    "    reg_accuracy = 100-mape\n",
    "\n",
    "    return {\n",
    "        \"Loss\": loss,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"MAPE\": mape,\n",
    "        \"Regression Accuracy\": reg_accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments - Predicint June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\4240801089.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[columns_norm] = scaler.fit_transform(train_data[columns_norm])\n",
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\4240801089.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[columns_norm] = scaler.transform(test_data[columns_norm])\n",
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\4240801089.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[columns_norm] = scaler.fit_transform(train_data[columns_norm])\n",
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\4240801089.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[columns_norm] = scaler.transform(test_data[columns_norm])\n",
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model (April-May --> June, ReLU)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 23ms/step - loss: 2.4596e-04 - val_loss: 7.4492e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 23ms/step - loss: 2.4986e-04 - val_loss: 7.3993e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 23ms/step - loss: 2.4769e-04 - val_loss: 7.3019e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 26ms/step - loss: 2.4920e-04 - val_loss: 7.2746e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 23ms/step - loss: 2.2706e-04 - val_loss: 7.1393e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 24ms/step - loss: 2.1195e-04 - val_loss: 7.3048e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 24ms/step - loss: 2.1777e-04 - val_loss: 6.9474e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 24ms/step - loss: 2.1605e-04 - val_loss: 5.1728e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 24ms/step - loss: 2.2461e-04 - val_loss: 6.8781e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 24ms/step - loss: 2.1737e-04 - val_loss: 5.6804e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 25ms/step - loss: 2.0148e-04 - val_loss: 6.9144e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 25ms/step - loss: 2.0105e-04 - val_loss: 5.8683e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 25ms/step - loss: 1.8389e-04 - val_loss: 5.0667e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 25ms/step - loss: 1.8523e-04 - val_loss: 5.0560e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 25ms/step - loss: 1.7019e-04 - val_loss: 5.9260e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 25ms/step - loss: 1.7006e-04 - val_loss: 5.1912e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 25ms/step - loss: 1.5836e-04 - val_loss: 4.3780e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 25ms/step - loss: 1.6387e-04 - val_loss: 6.9927e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 25ms/step - loss: 1.7279e-04 - val_loss: 4.3349e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 25ms/step - loss: 1.5345e-04 - val_loss: 4.6812e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - loss: 4.6181e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step\n",
      "Training model (May --> June, ReLU)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\1082494960.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  mape = np.mean(np.abs((test_targets - predictions.flatten()) / test_targets)) * 100\n",
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 25ms/step - loss: 2.2084e-04 - val_loss: 3.0286e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 27ms/step - loss: 2.2060e-04 - val_loss: 3.4352e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 25ms/step - loss: 2.1452e-04 - val_loss: 2.9588e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 26ms/step - loss: 2.1805e-04 - val_loss: 3.3857e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 26ms/step - loss: 2.1849e-04 - val_loss: 3.2040e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 25ms/step - loss: 2.1310e-04 - val_loss: 3.8397e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 25ms/step - loss: 2.0310e-04 - val_loss: 3.0862e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 26ms/step - loss: 2.1066e-04 - val_loss: 3.8319e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 24ms/step - loss: 1.9474e-04 - val_loss: 6.6039e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 25ms/step - loss: 2.2407e-04 - val_loss: 2.9252e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 25ms/step - loss: 1.9583e-04 - val_loss: 3.1654e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 28ms/step - loss: 1.9041e-04 - val_loss: 3.5553e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 31ms/step - loss: 1.8507e-04 - val_loss: 2.9178e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 30ms/step - loss: 1.8023e-04 - val_loss: 3.3117e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 32ms/step - loss: 1.8585e-04 - val_loss: 2.7723e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 31ms/step - loss: 2.1174e-04 - val_loss: 3.0690e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 32ms/step - loss: 0.0142 - val_loss: 0.0036\n",
      "Epoch 18/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 31ms/step - loss: 0.0352 - val_loss: 7.8318e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 31ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 20/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 32ms/step - loss: 0.0243 - val_loss: 7.6460e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - loss: 7.6414e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step\n",
      "Training model (April-May --> June, Tanh)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\1082494960.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  mape = np.mean(np.abs((test_targets - predictions.flatten()) / test_targets)) * 100\n",
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1040s\u001b[0m 42ms/step - loss: 2.5669e-04 - val_loss: 7.3094e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m908s\u001b[0m 37ms/step - loss: 2.4038e-04 - val_loss: 7.7779e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m826s\u001b[0m 34ms/step - loss: 2.2321e-04 - val_loss: 5.2760e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 40ms/step - loss: 1.9373e-04 - val_loss: 6.1898e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m966s\u001b[0m 40ms/step - loss: 1.8178e-04 - val_loss: 4.6281e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m962s\u001b[0m 39ms/step - loss: 1.7384e-04 - val_loss: 3.8393e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m966s\u001b[0m 39ms/step - loss: 1.7103e-04 - val_loss: 4.3532e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m967s\u001b[0m 40ms/step - loss: 1.5187e-04 - val_loss: 3.9045e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m937s\u001b[0m 38ms/step - loss: 1.4333e-04 - val_loss: 4.3437e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m933s\u001b[0m 38ms/step - loss: 1.3075e-04 - val_loss: 3.6932e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m926s\u001b[0m 38ms/step - loss: 1.2649e-04 - val_loss: 5.9373e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m930s\u001b[0m 38ms/step - loss: 1.2498e-04 - val_loss: 3.2407e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 38ms/step - loss: 1.2473e-04 - val_loss: 5.0814e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 38ms/step - loss: 1.2171e-04 - val_loss: 3.2208e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m922s\u001b[0m 38ms/step - loss: 1.0940e-04 - val_loss: 4.0210e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m883s\u001b[0m 36ms/step - loss: 1.1002e-04 - val_loss: 3.3006e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m894s\u001b[0m 37ms/step - loss: 1.0054e-04 - val_loss: 3.2455e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m906s\u001b[0m 37ms/step - loss: 9.3157e-05 - val_loss: 2.4856e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m879s\u001b[0m 36ms/step - loss: 9.0221e-05 - val_loss: 2.8582e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m24453/24453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m924s\u001b[0m 38ms/step - loss: 9.5561e-05 - val_loss: 2.9933e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - loss: 2.2531e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step\n",
      "Training model (May --> June, Tanh)\n",
      "Epoch 1/20\n",
      "\u001b[1m    1/12426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20:44\u001b[0m 100ms/step - loss: 6.5865e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\1082494960.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  mape = np.mean(np.abs((test_targets - predictions.flatten()) / test_targets)) * 100\n",
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 40ms/step - loss: 1.3769e-04 - val_loss: 2.8178e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 42ms/step - loss: 1.2908e-04 - val_loss: 1.3108e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 41ms/step - loss: 1.2153e-04 - val_loss: 2.1097e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 41ms/step - loss: 1.2799e-04 - val_loss: 1.6182e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 40ms/step - loss: 1.1925e-04 - val_loss: 1.9673e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 41ms/step - loss: 1.1647e-04 - val_loss: 1.5183e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 41ms/step - loss: 1.0862e-04 - val_loss: 1.4615e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 40ms/step - loss: 1.0775e-04 - val_loss: 1.3246e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 40ms/step - loss: 1.1788e-04 - val_loss: 1.3667e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 39ms/step - loss: 1.1574e-04 - val_loss: 1.4617e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 39ms/step - loss: 1.1352e-04 - val_loss: 2.1294e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 37ms/step - loss: 1.0815e-04 - val_loss: 1.1107e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 37ms/step - loss: 1.0233e-04 - val_loss: 1.1965e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 38ms/step - loss: 9.7670e-05 - val_loss: 1.2709e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 36ms/step - loss: 9.8719e-05 - val_loss: 1.2323e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 31ms/step - loss: 9.3487e-05 - val_loss: 1.1349e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 27ms/step - loss: 9.6274e-05 - val_loss: 1.2908e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 28ms/step - loss: 8.5916e-05 - val_loss: 1.1499e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 28ms/step - loss: 8.6343e-05 - val_loss: 1.1838e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 28ms/step - loss: 9.0838e-05 - val_loss: 1.2653e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 9.0270e-05\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\1082494960.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  mape = np.mean(np.abs((test_targets - predictions.flatten()) / test_targets)) * 100\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 24\n",
    "batch_size = 64\n",
    "epochs = 50  \n",
    "\n",
    " \n",
    "\n",
    "train_may, test_june, scaler = prepare_data(df_omie_labelled, train_months=[5], test_month=6)  \n",
    "# train_april_may, _, _ = prepare_data(df_omie_labelled, train_months=[4, 5], test_month=6)\n",
    "\n",
    "# model_relu= build_lstm_model([30, 30, 20], [\"relu\", \"relu\", \"relu\"], (sequence_length, 2))\n",
    "model_tanh= build_lstm_model([64, 64, 32], [\"tanh\", \"tanh\", \"tanh\"], (sequence_length, 2))\n",
    "# model_relu_tanh = build_lstm_model([50, 50, 30], [\"tanh\", \"tanh\", \"relu\"], (sequence_length, 2))\n",
    "\n",
    "# print(\"Training model (April-May --> June, ReLU)\")  \n",
    "# results_relu_am = train_evaluate_model(model_relu, train_april_may, test_june, sequence_length, batch_size, epochs)\n",
    "\n",
    "# print(\"Training model (May --> June, ReLU)\")   \n",
    "# results_relu_m = train_evaluate_model(model_relu, train_may, test_june, sequence_length, batch_size, epochs)\n",
    "  \n",
    "# print(\"Training model (April-May --> June, Tanh)\")\n",
    "# results_tanh_am = train_evaluate_model(model_tanh, train_april_may, test_june, sequence_length, batch_size, epochs)\n",
    "\n",
    "print(\"Training model (May --> June, Tanh)\")\n",
    "results_tanh_m = train_evaluate_model(model_tanh, train_may, test_june, sequence_length, batch_size, epochs)\n",
    "\n",
    "\n",
    "# print(\"Training model (May --> June, Tanh/Relu)\")\n",
    "# results_relu_tanh_m = train_evaluate_model(model_relu_tanh, train_may, test_june, sequence_length, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_relu_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_relu_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_tanh_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 0.0001265329192392528,\n",
       " 'MAE': 0.004819060364125532,\n",
       " 'RMSE': 0.011248683749875237,\n",
       " 'R2': 0.8340372729665887,\n",
       " 'MAPE': inf}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tanh_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_tanh_m (epochs=20, sequence_length = 24, batch_size = 64, LSTM units=[50, 50, 30], learning rate=0.01): \n",
    "\n",
    "{'Loss': 0.0001265329192392528,\n",
    "\n",
    " 'MAE': 0.004819060364125532,\n",
    "\n",
    " 'RMSE': 0.011248683749875237,\n",
    "\n",
    " 'R2': 0.8340372729665887,\n",
    " \n",
    " 'MAPE': inf}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
