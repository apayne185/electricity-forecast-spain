{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to use aprils data too to try to predict junes, however it considerably lowered the quality of the model: \n",
    "\n",
    "Loss: 0.0002993\n",
    "\n",
    "MAE: 0.005401\n",
    "\n",
    "RMSE: 0.01730\n",
    "\n",
    "R²: 0.6074\n",
    "\n",
    "Due to this, I am no longer using the models for it. However, may is an uncertain choice for predicting july's data. \n",
    "\n",
    "I also ruled out using relu, as it did not yield the results of tanh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import Sequence \n",
    "from tensorflow.keras.optimizers import Adam     \n",
    " \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,  mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omie_labelled = pd.read_csv('data\\\\df_omie_labelled.csv')\n",
    "filtered_categories = pd.read_csv('data\\\\filtered_categories.csv')\n",
    "df_omie_blind = pd.read_csv('data\\\\df_omie_blind.csv')\n",
    "unit_list = pd.read_csv('data\\\\unit_list.csv')\n",
    "\n",
    "\n",
    "unit_list[\"Tecnología\"] = unit_list[\"Tecnología\"].fillna(\"Unknown\")\n",
    "\n",
    "df_omie_labelled['fechaHora'] = pd.to_datetime(df_omie_labelled['fechaHora'])\n",
    "df_omie_blind['fechaHora'] = pd.to_datetime(df_omie_blind['fechaHora'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, train_months, test_month, columns_norm=[\"PrecEuro\", \"Energia\"]):\n",
    "    train_data = df[df['fechaHora'].dt.month.isin(train_months)]\n",
    "    test_data = df[df['fechaHora'].dt.month == test_month]\n",
    "     \n",
    "     \n",
    "    #fit scaler only on training data    \n",
    "    scaler = MinMaxScaler()  \n",
    "    train_data[columns_norm] = scaler.fit_transform(train_data[columns_norm])\n",
    "    test_data[columns_norm] = scaler.transform(test_data[columns_norm])\n",
    "    \n",
    "\n",
    "    return train_data, test_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergySequence(Sequence): \n",
    "    def __init__(self, data, target_col, sequence_length=24, batch_size=32):\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "         \n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.sequence_length) // self.batch_size \n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        X, y = [], []\n",
    "   \n",
    "\n",
    "        for i in range(start_idx, end_idx):\n",
    "            if i + self.sequence_length < len(self.data):\n",
    "                X.append(self.data.iloc[i : i + self.sequence_length].values)\n",
    "                y.append(self.data.iloc[i + self.sequence_length, self.target_col])\n",
    "\n",
    "        return np.array(X), np.array(y)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build Models \n",
    "(relu, tanh, combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(units, activations, input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(units[0], activation=activations[0], return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units[1], activation=activations[1]),\n",
    "        Dropout(0.2),\n",
    "        Dense(units[2], activation=activations[2]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)  \n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate Models\n",
    "\n",
    "trains on april-may, may data to predict junes data. From here I will determine which model (am/m using relu/tanh/combination) is best to predict july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(actual, predicted):\n",
    "    return 100 * np.mean(2 * np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted) + 1e-8))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, train_data, test_data, sequence_length, batch_size, epochs):\n",
    "    \n",
    "    train_generator = EnergySequence(train_data[['Energia', 'PrecEuro']], target_col=0, sequence_length=sequence_length, batch_size=batch_size)\n",
    "    test_generator = EnergySequence(test_data[['Energia', 'PrecEuro']], target_col=0, sequence_length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "    history = model.fit(train_generator, validation_data=test_generator, epochs=epochs, verbose=1)\n",
    "\n",
    "    loss = model.evaluate(test_generator)\n",
    "    \n",
    "    predictions = model.predict(test_generator)\n",
    "\n",
    "\n",
    "    test_targets = test_data['Energia'].iloc[sequence_length:sequence_length+len(predictions)].values\n",
    "    test_dates = test_data['fechaHora'].iloc[sequence_length:sequence_length+len(predictions)].values\n",
    "    \n",
    "    # error metrics\n",
    "    mae = mean_absolute_error(test_targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(test_targets, predictions))\n",
    "    r2 = r2_score(test_targets, predictions)\n",
    "    # mape = np.mean(np.abs((test_targets - predictions.flatten()) / test_targets)) * 100  \n",
    "    mape = smape(test_targets, predictions.flatten())\n",
    "    reg_accuracy = 100-mape\n",
    "\n",
    "    return {\n",
    "        \"Loss\": loss,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"MAPE\": mape,\n",
    "        \"Regression Accuracy\": reg_accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments - Predicint June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\4240801089.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[columns_norm] = scaler.fit_transform(train_data[columns_norm])\n",
      "C:\\Users\\annap\\AppData\\Local\\Temp\\ipykernel_9928\\4240801089.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[columns_norm] = scaler.transform(test_data[columns_norm])\n",
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model (May --> June, Tanh)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 36ms/step - loss: 3.4867e-04 - val_loss: 7.4617e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 43ms/step - loss: 3.4926e-04 - val_loss: 7.2121e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 32ms/step - loss: 3.2335e-04 - val_loss: 7.2255e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 39ms/step - loss: 3.0777e-04 - val_loss: 6.6338e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 67ms/step - loss: 2.7409e-04 - val_loss: 4.7246e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3481s\u001b[0m 280ms/step - loss: 2.7319e-04 - val_loss: 5.3459e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m853s\u001b[0m 69ms/step - loss: 2.7574e-04 - val_loss: 4.0667e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m893s\u001b[0m 72ms/step - loss: 2.4505e-04 - val_loss: 3.8235e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 70ms/step - loss: 2.2063e-04 - val_loss: 2.9959e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 72ms/step - loss: 2.0336e-04 - val_loss: 3.2758e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 68ms/step - loss: 2.0702e-04 - val_loss: 3.1012e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m870s\u001b[0m 70ms/step - loss: 1.9002e-04 - val_loss: 2.6611e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 71ms/step - loss: 1.7959e-04 - val_loss: 2.6562e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66180s\u001b[0m 5s/step - loss: 1.7828e-04 - val_loss: 2.6461e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 40ms/step - loss: 1.7440e-04 - val_loss: 2.5558e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 46ms/step - loss: 1.6508e-04 - val_loss: 3.3497e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 42ms/step - loss: 1.5657e-04 - val_loss: 3.6887e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 38ms/step - loss: 1.5294e-04 - val_loss: 2.8247e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 36ms/step - loss: 1.4631e-04 - val_loss: 3.2831e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 35ms/step - loss: 1.4288e-04 - val_loss: 1.7453e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 35ms/step - loss: 1.4086e-04 - val_loss: 1.9261e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 33ms/step - loss: 1.3656e-04 - val_loss: 1.8382e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 35ms/step - loss: 1.3158e-04 - val_loss: 1.6050e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 36ms/step - loss: 1.3102e-04 - val_loss: 3.1674e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 35ms/step - loss: 1.3612e-04 - val_loss: 1.5420e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 35ms/step - loss: 1.2835e-04 - val_loss: 1.4639e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 35ms/step - loss: 1.2666e-04 - val_loss: 1.7696e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 34ms/step - loss: 1.2919e-04 - val_loss: 1.5479e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 35ms/step - loss: 1.2017e-04 - val_loss: 1.2817e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 35ms/step - loss: 1.2276e-04 - val_loss: 1.4345e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 35ms/step - loss: 1.2592e-04 - val_loss: 1.8606e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 34ms/step - loss: 1.2133e-04 - val_loss: 1.5991e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 34ms/step - loss: 1.1908e-04 - val_loss: 1.4255e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 35ms/step - loss: 1.1422e-04 - val_loss: 1.4585e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 34ms/step - loss: 1.1232e-04 - val_loss: 1.4318e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 34ms/step - loss: 1.1030e-04 - val_loss: 1.1658e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 34ms/step - loss: 1.1682e-04 - val_loss: 1.2993e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 29ms/step - loss: 1.0857e-04 - val_loss: 1.1991e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 28ms/step - loss: 1.0917e-04 - val_loss: 1.3164e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 27ms/step - loss: 1.0596e-04 - val_loss: 1.1867e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 28ms/step - loss: 1.0371e-04 - val_loss: 1.0184e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 27ms/step - loss: 1.0674e-04 - val_loss: 1.4945e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 27ms/step - loss: 1.1175e-04 - val_loss: 1.5924e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 28ms/step - loss: 9.9174e-05 - val_loss: 1.1645e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 28ms/step - loss: 1.0700e-04 - val_loss: 1.1505e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 28ms/step - loss: 9.7722e-05 - val_loss: 1.0777e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 28ms/step - loss: 9.6686e-05 - val_loss: 8.9756e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 28ms/step - loss: 9.9664e-05 - val_loss: 1.1877e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 30ms/step - loss: 9.4530e-05 - val_loss: 1.2168e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m12426/12426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 29ms/step - loss: 9.0615e-05 - val_loss: 1.0761e-04\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 8.5811e-05\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 24\n",
    "batch_size = 64\n",
    "epochs = 50  \n",
    "\n",
    " \n",
    "\n",
    "train_may, test_june, scaler = prepare_data(df_omie_labelled, train_months=[5], test_month=6)  \n",
    "# train_april_may, _, _ = prepare_data(df_omie_labelled, train_months=[4, 5], test_month=6)\n",
    "\n",
    "# model_relu= build_lstm_model([30, 30, 20], [\"relu\", \"relu\", \"relu\"], (sequence_length, 2))\n",
    "model_tanh= build_lstm_model([64, 64, 32], [\"tanh\", \"tanh\", \"tanh\"], (sequence_length, 2))\n",
    "# model_relu_tanh = build_lstm_model([50, 50, 30], [\"tanh\", \"tanh\", \"relu\"], (sequence_length, 2))\n",
    "\n",
    "# print(\"Training model (April-May --> June, ReLU)\")  \n",
    "# results_relu_am = train_evaluate_model(model_relu, train_april_may, test_june, sequence_length, batch_size, epochs)\n",
    "\n",
    "# print(\"Training model (May --> June, ReLU)\")   \n",
    "# results_relu_m = train_evaluate_model(model_relu, train_may, test_june, sequence_length, batch_size, epochs)\n",
    "  \n",
    "# print(\"Training model (April-May --> June, Tanh)\")\n",
    "# results_tanh_am = train_evaluate_model(model_tanh, train_april_may, test_june, sequence_length, batch_size, epochs)\n",
    "\n",
    "print(\"Training model (May --> June, Tanh)\")\n",
    "results_tanh_m = train_evaluate_model(model_tanh, train_may, test_june, sequence_length, batch_size, epochs)\n",
    "\n",
    "\n",
    "# print(\"Training model (May --> June, Tanh/Relu)\")\n",
    "# results_relu_tanh_m = train_evaluate_model(model_relu_tanh, train_may, test_june, sequence_length, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_relu_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_relu_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_tanh_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 0.00010760701115941629,\n",
       " 'MAE': 0.004284586236129825,\n",
       " 'RMSE': 0.010373377932773709,\n",
       " 'R2': 0.8588608329819016,\n",
       " 'MAPE': 133.4399663867288,\n",
       " 'Regression Accuracy': -33.4399663867288}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tanh_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_tanh_m (epochs=20, sequence_length = 24, batch_size = 64, LSTM units=[50, 50, 30], learning rate=0.01): \n",
    "\n",
    "{'Loss': 0.0001265329192392528,\n",
    "\n",
    " 'MAE': 0.004819060364125532,\n",
    "\n",
    " 'RMSE': 0.011248683749875237,\n",
    "\n",
    " 'R2': 0.8340372729665887,\n",
    " \n",
    " 'MAPE': inf}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
