{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to use aprils data too to try to predict junes, however it considerably lowered the quality of the model: \n",
    "\n",
    "Loss: 0.0002993\n",
    "\n",
    "MAE: 0.005401\n",
    "\n",
    "RMSE: 0.01730\n",
    "\n",
    "R²: 0.6074\n",
    "\n",
    "Due to this, I am no longer using the models for it. However, may is an uncertain choice for predicting july's data. \n",
    "\n",
    "I also ruled out using relu, as it did not yield the results of tanh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import Sequence \n",
    "from tensorflow.keras.optimizers import Adam    \n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "# from tensorflow.keras.losses import Huber\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,  mean_squared_log_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codigo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>fechaHora</th>\n",
       "      <th>PrecEuro</th>\n",
       "      <th>Energia</th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Descripción</th>\n",
       "      <th>Agente</th>\n",
       "      <th>Porcentaje_Propiedad</th>\n",
       "      <th>Tipo_Unidad</th>\n",
       "      <th>Zona/Frontera</th>\n",
       "      <th>Tecnología</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADXRE01</td>\n",
       "      <td>PREAL1</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Solar</td>\n",
       "      <td>PREAL1</td>\n",
       "      <td>AUDAX RENOVABLES S.A.</td>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERACION</td>\n",
       "      <td>ZONA ESPAÑOLA</td>\n",
       "      <td>RE Mercado Solar Fotovoltáica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADXVD03</td>\n",
       "      <td>PV ALARCOS</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Solar</td>\n",
       "      <td>PV ALARCOS</td>\n",
       "      <td>AUDAX RENOVABLES S.A.</td>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERACION</td>\n",
       "      <td>ZONA ESPAÑOLA</td>\n",
       "      <td>RE Mercado Solar Fotovoltáica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADXVD04</td>\n",
       "      <td>EOLICA AUDAX</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>Eolica</td>\n",
       "      <td>EOLICA AUDAX</td>\n",
       "      <td>AUDAX RENOVABLES S.A.</td>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERACION</td>\n",
       "      <td>ZONA ESPAÑOLA</td>\n",
       "      <td>RE Mercado Eólica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIRVD01</td>\n",
       "      <td>AIRVD01</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Solar</td>\n",
       "      <td>AIRVD01</td>\n",
       "      <td>AIRE LIMPIO S.L.</td>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERACION</td>\n",
       "      <td>ZONA ESPAÑOLA</td>\n",
       "      <td>RE Mercado Solar Fotovoltáica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AIRVD02</td>\n",
       "      <td>AIRVD02</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Solar</td>\n",
       "      <td>AIRVD02</td>\n",
       "      <td>AIRE LIMPIO S.L.</td>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERACION</td>\n",
       "      <td>ZONA ESPAÑOLA</td>\n",
       "      <td>RE Mercado Solar Fotovoltáica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Codigo   Descripcion  fechaHora  PrecEuro  Energia Categoria  \\\n",
       "0  ADXRE01        PREAL1 2024-02-29       0.0      0.0     Solar   \n",
       "1  ADXVD03    PV ALARCOS 2024-02-29       0.0      0.0     Solar   \n",
       "2  ADXVD04  EOLICA AUDAX 2024-02-29      -0.1      2.7    Eolica   \n",
       "3  AIRVD01       AIRVD01 2024-02-29       0.0      0.0     Solar   \n",
       "4  AIRVD02       AIRVD02 2024-02-29       0.0      0.0     Solar   \n",
       "\n",
       "    Descripción                 Agente  Porcentaje_Propiedad Tipo_Unidad  \\\n",
       "0        PREAL1  AUDAX RENOVABLES S.A.                 100.0  GENERACION   \n",
       "1    PV ALARCOS  AUDAX RENOVABLES S.A.                 100.0  GENERACION   \n",
       "2  EOLICA AUDAX  AUDAX RENOVABLES S.A.                 100.0  GENERACION   \n",
       "3       AIRVD01       AIRE LIMPIO S.L.                 100.0  GENERACION   \n",
       "4       AIRVD02       AIRE LIMPIO S.L.                 100.0  GENERACION   \n",
       "\n",
       "   Zona/Frontera                     Tecnología  \n",
       "0  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
       "1  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
       "2  ZONA ESPAÑOLA              RE Mercado Eólica  \n",
       "3  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
       "4  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_omie_labelled = pd.read_csv('data\\\\df_omie_labelled.csv')\n",
    "unit_list = pd.read_csv('data\\\\unit_list.csv')\n",
    "filtered_categories = pd.read_csv('data\\\\filtered_categories.csv')\n",
    "df_omie_blind = pd.read_csv('data\\\\df_omie_blind.csv')\n",
    "\n",
    "df_omie_labelled['fechaHora'] = pd.to_datetime(df_omie_labelled['fechaHora'])\n",
    "\n",
    "df = pd.merge(df_omie_labelled, filtered_categories, on='Codigo', how='left')\n",
    "df = pd.merge(df, unit_list, on='Codigo', how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.409526e+06\n",
      "mean     1.386135e+01\n",
      "std      7.212333e+01\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      8.000000e-01\n",
      "75%      9.100000e+00\n",
      "max      5.047000e+03\n",
      "Name: Energia, dtype: float64\n",
      "          Codigo                     Descripcion           fechaHora  \\\n",
      "0        ADXRE01                          PREAL1 2024-02-29 00:00:00   \n",
      "1        ADXVD03                      PV ALARCOS 2024-02-29 00:00:00   \n",
      "3        AIRVD01                         AIRVD01 2024-02-29 00:00:00   \n",
      "4        AIRVD02                         AIRVD02 2024-02-29 00:00:00   \n",
      "5        ALDRE01                ALDRO GENERACION 2024-02-29 00:00:00   \n",
      "...          ...                             ...                 ...   \n",
      "2409516  WMVD155  VENTA EXCED AUTOPROD WMARK 155 2024-06-01 22:00:00   \n",
      "2409517  WMVD166  VENTA EXCED AUTOPROD WMARK 166 2024-06-01 22:00:00   \n",
      "2409522  WMVD206  VENTA EXCED AUTOPROD WMARK 206 2024-06-01 22:00:00   \n",
      "2409524  ZRMRE01                  UOF CERVANTINA 2024-06-01 22:00:00   \n",
      "2409525  ZRMRE03                    UOF HYANOR 2 2024-06-01 22:00:00   \n",
      "\n",
      "         PrecEuro  Energia Categoria                     Descripción  \\\n",
      "0             0.0      0.0     Solar                          PREAL1   \n",
      "1             0.0      0.0     Solar                      PV ALARCOS   \n",
      "3             0.0      0.0     Solar                         AIRVD01   \n",
      "4             0.0      0.0     Solar                         AIRVD02   \n",
      "5             0.0      0.0     Solar                ALDRO GENERACION   \n",
      "...           ...      ...       ...                             ...   \n",
      "2409516       0.0      0.0     Solar  VENTA EXCED AUTOPROD WMARK 155   \n",
      "2409517      -4.0      0.1     Solar  VENTA EXCED AUTOPROD WMARK 166   \n",
      "2409522       0.0      0.0     Solar  VENTA EXCED AUTOPROD WMARK 206   \n",
      "2409524       0.0      0.0     Solar                  UOF CERVANTINA   \n",
      "2409525       0.0      0.0     Solar                    UOF HYANOR 2   \n",
      "\n",
      "                                             Agente  Porcentaje_Propiedad  \\\n",
      "0                             AUDAX RENOVABLES S.A.                 100.0   \n",
      "1                             AUDAX RENOVABLES S.A.                 100.0   \n",
      "3                                  AIRE LIMPIO S.L.                 100.0   \n",
      "4                                  AIRE LIMPIO S.L.                 100.0   \n",
      "5                      ENI PLENITUDE IBERIA, S.L.U.                 100.0   \n",
      "...                                             ...                   ...   \n",
      "2409516  WIND TO MARKET (ACT.: COMERCIALIZACIÓN RE)                 100.0   \n",
      "2409517  WIND TO MARKET (ACT.: COMERCIALIZACIÓN RE)                 100.0   \n",
      "2409522  WIND TO MARKET (ACT.: COMERCIALIZACIÓN RE)                 100.0   \n",
      "2409524  COMERCIALIZADORA ELECTRICA COMUNIDAD SOLAR                 100.0   \n",
      "2409525  COMERCIALIZADORA ELECTRICA COMUNIDAD SOLAR                 100.0   \n",
      "\n",
      "        Tipo_Unidad  Zona/Frontera                     Tecnología  \n",
      "0        GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "1        GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "3        GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "4        GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "5        GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "...             ...            ...                            ...  \n",
      "2409516  GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "2409517  GENERACION  ZONA ESPAÑOLA       RE Mercado Solar Térmica  \n",
      "2409522  GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "2409524  GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "2409525  GENERACION  ZONA ESPAÑOLA  RE Mercado Solar Fotovoltáica  \n",
      "\n",
      "[1229185 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df['Energia'].describe())\n",
    "print(df[df['Energia'] < 1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, sequence_length=24, test_size=1, exogenous_features=['PrecEuro']):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_copy['log_Energia'] = np.log1p(df_copy['Energia'])\n",
    "    \n",
    "    df_copy['fechaHora'] = pd.to_datetime(df_copy['fechaHora'])\n",
    "    df_copy['Hora'] = df_copy['fechaHora'].dt.hour\n",
    "    df_copy['DiaSemana'] = df_copy['fechaHora'].dt.weekday\n",
    "    df_copy['Mes'] = df_copy['fechaHora'].dt.month\n",
    "\n",
    "    categorical_columns = ['Categoria', 'Agente', 'Tipo_Unidad', 'Zona/Frontera', 'Tecnología']\n",
    "    df_copy = pd.get_dummies(df_copy, columns=categorical_columns, drop_first=True)\n",
    "    \n",
    "    features = ['Categoria', 'Tecnología', 'Hora', 'DiaSemana', 'Mes'] + exogenous_features\n",
    "\n",
    "    # numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    #fit scaler only on training data    \n",
    "    scaler = StandardScaler()\n",
    "    feature_columns = [col for col in df_copy.columns if col not in ['Fecha', 'Energia', 'log_Energia', 'Codigo', 'fechaHora']]\n",
    "    df_copy[feature_columns] = scaler.fit_transform(df_copy[feature_columns])\n",
    "    # df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergySequence(Sequence): \n",
    "    def __init__(self, df, target_col, sequence_length=24, batch_size=32, n_plants=300,  is_test=False):\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.n_plants = n_plants\n",
    "        self.is_test = is_test\n",
    "        self.plants = df['Codigo'].unique()\n",
    "         \n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return (len(self.data) - self.sequence_length) // self.batch_size \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df)/ self.batch_size))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        plant_code = self.plants[idx % len(self.plants)]\n",
    "        plant_data = self.df[self.df['Codigo'] == plant_code]\n",
    "        batch_features = []\n",
    "        batch_target = []\n",
    "   \n",
    "\n",
    "        for i in range(self.sequence_length, len(plant_data)):\n",
    "            if i + self.sequence_length < len(self.df):\n",
    "                sequence = self.df.iloc[i : i + self.sequence_length]\n",
    "                features = sequence.drop(columns=['Energia', 'log_Energia', 'fechaHora'])\n",
    "                target = sequence['log_Energia'].iloc[-1]  \n",
    "                \n",
    "                batch_features.append(features.values)\n",
    "                batch_target.append(target)\n",
    "                    \n",
    "        return np.array(batch_features), np.array(batch_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build Models \n",
    "(relu, tanh, combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh',return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='tanh',),\n",
    "        Dropout(0.2),\n",
    "        # Dense(units[2], activation=activations[2]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)  \n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate Models\n",
    "\n",
    "trains on april-may, may data to predict junes data. From here I will determine which model (am/m using relu/tanh/combination) is best to predict july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(actual, predicted):\n",
    "    return 100 * np.mean(2 * np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted) + 1e-8))  \n",
    "\n",
    "\n",
    "def evaluate_model(model, test_generator):\n",
    "    # train_generator = EnergySequence(train_data[['Energia', 'PrecEuro']], target_col=0, sequence_length=sequence_length, batch_size=batch_size)\n",
    "    # test_generator = EnergySequence(test_data[['Energia', 'PrecEuro']], target_col=0, sequence_length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "    # history = model.fit(train_generator, validation_data=test_generator, epochs=epochs, verbose=1)\n",
    "    # loss = model.evaluate(test_generator)\n",
    "    \n",
    "    predictions = model.predict(test_generator, verbose=0)\n",
    "    # true_values = test_generator.df['log_Energia'].iloc[test_generator.indexes[-len(predictions):]].values\n",
    "    true_values = test_generator.data['log_Energia'].iloc[-len(predictions):].values\n",
    "\n",
    "    predictions = np.expm1(predictions)\n",
    "    true_values = np.expm1(true_values)\n",
    "\n",
    "    # test_targets = test_data['Energia'].iloc[sequence_length:sequence_length+len(predictions)].values\n",
    "    # test_dates = test_data['fechaHora'].iloc[sequence_length:sequence_length+len(predictions)].values\n",
    "    \n",
    "    # error metrics\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    mape = smape(true_values, predictions.flatten())\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"MAPE\": mape,\n",
    "        \"Predictions\": predictions.flatten(),  \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Codigo', 'Descripcion', 'fechaHora', 'PrecEuro', 'Energia',\n",
      "       'Categoria', 'Descripción', 'Agente', 'Porcentaje_Propiedad',\n",
      "       'Tipo_Unidad', 'Zona/Frontera', 'Tecnología', 'log_Energia', 'Hora',\n",
      "       'DiaSemana', 'Mes'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'PREAL1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 60\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m---> 60\u001b[0m prepared_df, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexogenous_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrecEuro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[0;32m     63\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnergia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodigo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfechaHora\u001b[39m\u001b[38;5;124m'\u001b[39m])  \n",
      "Cell \u001b[1;32mIn[87], line 21\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(df, sequence_length, test_size, exogenous_features)\u001b[0m\n\u001b[0;32m     19\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m     20\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_copy\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnergia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_Energia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodigo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfechaHora\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 21\u001b[0m df_copy[feature_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df, scaler\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:839\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:875\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    874\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 875\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:836\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    835\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[1;32m--> 836\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'PREAL1'"
     ]
    }
   ],
   "source": [
    "# def cross_validate_model(df, n_splits=3, sequence_length=24):\n",
    "#     tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "#     for train_idx, test_idx in tscv.split(df):\n",
    "#         train_data, test_data = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        \n",
    "#         # train_generator = EnergySequence(train_data, sequence_length=sequence_length)\n",
    "#         train_generator = EnergySequence(train_data, target_col='log_Energia', sequence_length=sequence_length, batch_size=32, n_plants=300)\n",
    "#         test_generator = EnergySequence(test_data, target_col='log_Energia', sequence_length=sequence_length, is_test=True)\n",
    "        \n",
    "#         model = build_lstm_model((train_generator.sequence_length, train_data.shape[1] - 3))\n",
    "        \n",
    "#         model.fit(train_generator, epochs=5, batch_size=32, verbose=1)\n",
    "        \n",
    "#         mae, rmse, r2, smape = evaluate_model(model, test_generator)\n",
    "#         print(f\"MAE: {mae}, RMSE: {rmse}, R2: {r2}, SMAPE: {smape}\")\n",
    "\n",
    "def cross_validate_model(df, n_splits=3, sequence_length=24):\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.drop(columns=['Descripcion', 'fechaHora'])\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    categorical_columns = ['Categoria', 'Agente', 'Tipo_Unidad', 'Zona/Frontera', 'Tecnología']\n",
    "    df_copy = pd.get_dummies(df_copy, columns=categorical_columns, drop_first=True)\n",
    "    \n",
    "    # Keep 'Codigo' for prediction, but exclude it from the feature set\n",
    "    feature_columns = [col for col in df_copy.columns if col not in ['Energia', 'log_Energia', 'Codigo']]  # Exclude target and 'Codigo'\n",
    "    \n",
    "    # Handle time-series split and training\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for train_idx, test_idx in tscv.split(df_copy):\n",
    "        train_data, test_data = df_copy.iloc[train_idx], df_copy.iloc[test_idx]\n",
    "        \n",
    "        # Make sure only numeric columns are used for features\n",
    "        train_data = train_data.select_dtypes(include=[np.number])\n",
    "        test_data = test_data.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Now, create the generators\n",
    "        train_generator = EnergySequence(train_data, target_col='log_Energia', sequence_length=sequence_length, batch_size=32, n_plants=300)\n",
    "        test_generator = EnergySequence(test_data, target_col='log_Energia', sequence_length=sequence_length, is_test=True)\n",
    "        \n",
    "        # Build and train the model\n",
    "        model = build_lstm_model((train_generator.sequence_length, train_data.shape[1] - 3))\n",
    "        model.fit(train_generator, epochs=5, batch_size=32, verbose=1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae, rmse, r2, smape = evaluate_model(model, test_generator)\n",
    "        print(f\"MAE: {mae}, RMSE: {rmse}, R2: {r2}, SMAPE: {smape}\")\n",
    "        \n",
    "        # After prediction, reattach 'Codigo' to the results for prediction purposes\n",
    "        predicted_values = model.predict(test_generator)\n",
    "        test_data['Predicted_Energia'] = predicted_values\n",
    "        \n",
    "        # Keep 'Codigo' in the final prediction output\n",
    "        predictions = test_data[['Codigo', 'Predicted_Energia']]\n",
    "        print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "prepared_df, scaler = prepare_data(df, sequence_length=24, test_size=1, exogenous_features=['PrecEuro'])\n",
    "\n",
    "sequence_length = 24\n",
    "X = df.drop(columns=['Energia', 'Codigo', 'fechaHora'])  \n",
    "y = df['Energia'] \n",
    "print(\"Shape of X before reshaping:\", X.shape)\n",
    "print(\"Shape of y before slicing:\", y.shape)\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "# numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "n_samples = X_scaled.shape[0] // sequence_length\n",
    "X= X_scaled[:n_samples * sequence_length]\n",
    "X= X.reshape((n_samples, sequence_length, X.shape[1]))\n",
    "y= y.iloc[sequence_length-1:n_samples * sequence_length:sequence_length].values\n",
    "\n",
    "print(\"Shape of X:\", X.shape)  # Expected: (100396, 24, 9)    \n",
    "print(\"Shape of y:\", y.shape)  # Expected: (100396,) \n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)  \n",
    "           \n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)  \n",
    " \n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "  \n",
    "print(\"X_train dtype:\", X_train.dtype)     \n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codigo                          object\n",
      "Descripcion                     object\n",
      "fechaHora               datetime64[ns]\n",
      "PrecEuro                       float64\n",
      "Energia                        float64\n",
      "Categoria                       object\n",
      "Descripción                     object\n",
      "Agente                          object\n",
      "Porcentaje_Propiedad           float64\n",
      "Tipo_Unidad                     object\n",
      "Zona/Frontera                   object\n",
      "Tecnología                      object\n",
      "log_Energia                    float64\n",
      "Hora                             int32\n",
      "DiaSemana                        int32\n",
      "Mes                              int32\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcross_validate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[83], line 12\u001b[0m, in \u001b[0;36mcross_validate_model\u001b[1;34m(df, n_splits, sequence_length)\u001b[0m\n\u001b[0;32m      8\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m EnergySequence(test_data, target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_Energia\u001b[39m\u001b[38;5;124m'\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39msequence_length, is_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m build_lstm_model((train_generator\u001b[38;5;241m.\u001b[39msequence_length, train_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m mae, rmse, r2, smape \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_generator)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SMAPE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optree\\ops.py:747\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    745\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    746\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "\n",
    "cross_validate_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before reshaping: (2409526, 10)\n",
      "Shape of y before slicing: (2409526,)\n",
      "Shape of X: (100396, 24, 2)\n",
      "Shape of y: (100396,)\n",
      "X_train dtype: float32\n",
      "y_train dtype: float32\n",
      "X_train dtype: float32\n",
      "y_train dtype: float32\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 25ms/step - loss: 5188.1152\n",
      "Epoch 2/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 27ms/step - loss: 4878.4912\n",
      "Epoch 3/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 23ms/step - loss: 5131.0024\n",
      "Epoch 4/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 19ms/step - loss: 5916.4756\n",
      "Epoch 5/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 20ms/step - loss: 6227.3403\n",
      "Epoch 6/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 20ms/step - loss: 4918.7920\n",
      "Epoch 7/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 20ms/step - loss: 5893.1675\n",
      "Epoch 8/8\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 20ms/step - loss: 5689.7109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22281478440>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sequence_length = 24\n",
    "X = df.drop(columns=['Energia', 'fechaHora'])  \n",
    "y = df['Energia'] \n",
    "print(\"Shape of X before reshaping:\", X.shape)\n",
    "print(\"Shape of y before slicing:\", y.shape)\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "# numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "n_samples = X_scaled.shape[0] // sequence_length\n",
    "X = X_scaled[:n_samples * sequence_length]\n",
    "X = X.reshape((n_samples, sequence_length, X.shape[1]))\n",
    "y = y.iloc[sequence_length-1:n_samples * sequence_length:sequence_length].values\n",
    "\n",
    "print(\"Shape of X:\", X.shape)  # Expected: (100396, 24, 9)\n",
    "print(\"Shape of y:\", y.shape)  # Expected: (100396,)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "\n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "model.fit(X_train, y_train, epochs=8, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fechaHora', 'Hora', 'DiaSemana', 'Mes', 'PrecEuro'], dtype='object')\n",
      "Shape of batch_features: (672, 24, 4)\n",
      "(672, 24, 4)\n",
      "Shape of batch_features: (672, 24, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch_features: (672, 24, 4)\n",
      "Shape of batch_features: (672, 24, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 4 and 2 for '{{node sequential_7_1/lstm_14_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_7_1/lstm_14_1/strided_slice_1, sequential_7_1/lstm_14_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [672,4], [2,256].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(672, 4), dtype=float32)\n  • states=('tf.Tensor(shape=(672, 64), dtype=float32)', 'tf.Tensor(shape=(672, 64), dtype=float32)')\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m batch_features, _ \u001b[38;5;241m=\u001b[39m forecast_generator[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Get a single batch\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 59\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforecast_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m df_future[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecast_Energia\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpm1(predictions)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_future[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfechaHora\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecast_Energia\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\annap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 4 and 2 for '{{node sequential_7_1/lstm_14_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_7_1/lstm_14_1/strided_slice_1, sequential_7_1/lstm_14_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [672,4], [2,256].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(672, 4), dtype=float32)\n  • states=('tf.Tensor(shape=(672, 64), dtype=float32)', 'tf.Tensor(shape=(672, 64), dtype=float32)')\n  • training=False"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch_features: (672, 24, 4)\n"
     ]
    }
   ],
   "source": [
    "class ForecastingSequence(Sequence):\n",
    "    def __init__(self, df, sequence_length=24, batch_size=32):\n",
    "        self.df = df\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if 'Codigo' in df.columns:\n",
    "            self.plants = df['Codigo'].unique()\n",
    "        else:\n",
    "            self.plants = ['default']  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "    \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if 'Codigo' in self.df.columns:\n",
    "            plant_code= self.plants[index % len(self.plants)]\n",
    "            plant_data= self.df[self.df['Codigo'] == plant_code]\n",
    "        else:\n",
    "            plant_data = self.df        #uses full dataset if no Codigo\n",
    "            \n",
    "        batch_features = []\n",
    "        \n",
    "        for idx in range(self.sequence_length, len(plant_data)):\n",
    "            sequence = plant_data.iloc[idx - self.sequence_length:idx]\n",
    "            features = sequence.drop(columns=['Energia', 'log_Energia', 'fechaHora'], errors='ignore')\n",
    "            \n",
    "            batch_features.append(features.values)\n",
    "            # batch_features.append(features.values.reshape(self.sequence_length, -1))\n",
    "        \n",
    "        batch_features = np.array(batch_features)\n",
    "        print(f\"Shape of batch_features: {batch_features.shape}\")\n",
    "        return np.array(batch_features), np.zeros(len(batch_features))\n",
    "\n",
    "\n",
    "future_dates = pd.date_range(start=\"2024-06-01\", periods=29 * 24, freq='H')\n",
    "\n",
    "df_future = pd.DataFrame({'fechaHora': future_dates})\n",
    "df_future['Hora'] = df_future['fechaHora'].dt.hour\n",
    "df_future['DiaSemana'] = df_future['fechaHora'].dt.weekday\n",
    "df_future['Mes'] = df_future['fechaHora'].dt.month\n",
    "\n",
    "\n",
    "df_future['PrecEuro'] = df_omie_labelled['PrecEuro'].mean()\n",
    "print(df_future.columns)\n",
    "\n",
    "\n",
    "scaler.fit(df_future[['PrecEuro', 'Hora', 'DiaSemana', 'Mes']])\n",
    "df_future[['PrecEuro', 'Hora', 'DiaSemana', 'Mes']] = scaler.transform(df_future[['PrecEuro', 'Hora', 'DiaSemana', 'Mes']])\n",
    "\n",
    "# df_future_reshaped = df_future[['PrecEuro', 'Hora', 'DiaSemana', 'Mes']].values\n",
    "# df_future_reshaped = df_future_reshaped.reshape(df_future_reshaped.shape[0], 1, df_future_reshaped.shape[1])\n",
    "\n",
    "forecast_generator = ForecastingSequence(df_future)\n",
    "# predictions = model.predict(forecast_generator, verbose=0)\n",
    "batch_features, _ = forecast_generator[0]  # Get a single batch\n",
    "batch_features = batch_features.reshape(-1, forecast_generator.sequence_length, batch_features.shape[-1])  # Adjust this to the expected shape\n",
    "print(f\"Shape of batch_features for prediction: {batch_features.shape}\")\n",
    "\n",
    "predictions = model.predict(batch_features, verbose=0)\n",
    "df_future['Forecast_Energia'] = np.expm1(predictions)\n",
    "\n",
    "print(df_future[['fechaHora', 'Forecast_Energia']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_future['fechaHora'], df_future['Forecast_Energia'], label='Forecasted Energy Supply')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Forecasted Energy (MWh)')\n",
    "plt.title('Energy Forecast for June 2024')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future.to_csv(\"energy_forecast_june_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_tanh_m (epochs=20, sequence_length = 24, batch_size = 64, LSTM units=[50, 50, 30], learning rate=0.01): \n",
    "\n",
    "{'Loss': 0.0001265329192392528,\n",
    "\n",
    " 'MAE': 0.004819060364125532,\n",
    "\n",
    " 'RMSE': 0.011248683749875237,\n",
    "\n",
    " 'R2': 0.8340372729665887,\n",
    " \n",
    " 'MAPE': inf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'Loss': 0.00010760701115941629,\n",
    " 'MAE': 0.004284586236129825,\n",
    " 'RMSE': 0.010373377932773709,\n",
    " 'R2': 0.8588608329819016,\n",
    " 'MAPE': 133.4399663867288,\n",
    " 'Regression Accuracy': -33.4399663867288}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
